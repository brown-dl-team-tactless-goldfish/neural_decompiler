func_1:
        pushq   %rbp
        movq    %rsp, %rbp
        pushq   %rbx
        subq    $72, %rsp
        movq    %rdi, -72(%rbp)
        movq    %rsi, -80(%rbp)
        movq    -72(%rbp), %rax
        addq    $1, %rax
        movzbl  (%rax), %eax
        movsbl  %al, %eax
        subl    $48, %eax
        movl    %eax, -36(%rbp)
        movq    -72(%rbp), %rax
        addq    $4, %rax
        movzbl  (%rax), %eax
        movsbl  %al, %eax
        subl    $48, %eax
        movl    %eax, -40(%rbp)
        movl    -40(%rbp), %eax
        subl    -36(%rbp), %eax
        addl    $1, %eax
        movl    %eax, -44(%rbp)
        movq    -72(%rbp), %rax
        addq    $3, %rax
        movzbl  (%rax), %ecx
        movsbw  %cl, %dx
        movl    %edx, %eax
        sall    $7, %eax
        subl    %edx, %eax
        shrw    $8, %ax
        movl    %eax, %edx
        sarb    $5, %dl
        movl    %ecx, %eax
        sarb    $7, %al
        subl    %eax, %edx
        movl    %edx, %eax
        sall    $6, %eax
        addl    %edx, %eax
        subl    %eax, %ecx
        movl    %ecx, %edx
        movsbl  %dl, %esi
        movq    -72(%rbp), %rax
        movzbl  (%rax), %ecx
        movsbw  %cl, %dx
        movl    %edx, %eax
        sall    $7, %eax
        subl    %edx, %eax
        shrw    $8, %ax
        movl    %eax, %edx
        sarb    $5, %dl
        movl    %ecx, %eax
        sarb    $7, %al
        subl    %eax, %edx
        movl    %edx, %eax
        sall    $6, %eax
        addl    %edx, %eax
        subl    %eax, %ecx
        movl    %ecx, %edx
        movsbl  %dl, %eax
        subl    %eax, %esi
        movl    %esi, %eax
        addl    $1, %eax
        movl    %eax, -48(%rbp)
        movl    -44(%rbp), %eax
        imull   -48(%rbp), %eax
        movl    %eax, -52(%rbp)
        movl    -52(%rbp), %eax
        cltq
        salq    $3, %rax
        movq    %rax, %rdi
        call    malloc
        movq    %rax, -64(%rbp)
        movl    $0, -20(%rbp)
        jmp     .L2
.L3:
        movl    -20(%rbp), %eax
        cltq
        leaq    0(,%rax,8), %rdx
        movq    -64(%rbp), %rax
        leaq    (%rdx,%rax), %rbx
        movl    $3, %edi
        call    malloc
        movq    %rax, (%rbx)
        addl    $1, -20(%rbp)
.L2:
        movl    -20(%rbp), %eax
        cmpl    -52(%rbp), %eax
        jl      .L3
        movq    -72(%rbp), %rax
        movzbl  (%rax), %eax
        movb    %al, -21(%rbp)
        movl    $0, -28(%rbp)
        jmp     .L4
.L7:
        movl    -36(%rbp), %eax
        movl    %eax, -32(%rbp)
        jmp     .L5
.L6:
        movl    -28(%rbp), %eax
        cltq
        leaq    0(,%rax,8), %rdx
        movq    -64(%rbp), %rax
        addq    %rdx, %rax
        movq    (%rax), %rax
        movzbl  -21(%rbp), %edx
        movb    %dl, (%rax)
        movl    -32(%rbp), %eax
        leal    48(%rax), %ecx
        movl    -28(%rbp), %eax
        cltq
        leaq    0(,%rax,8), %rdx
        movq    -64(%rbp), %rax
        addq    %rdx, %rax
        movq    (%rax), %rax
        addq    $1, %rax
        movl    %ecx, %edx
        movb    %dl, (%rax)
        movl    -28(%rbp), %eax
        cltq
        leaq    0(,%rax,8), %rdx
        movq    -64(%rbp), %rax
        addq    %rdx, %rax
        movq    (%rax), %rax
        addq    $2, %rax
        movb    $0, (%rax)
        addl    $1, -28(%rbp)
        addl    $1, -32(%rbp)
.L5:
        movl    -32(%rbp), %eax
        cmpl    -40(%rbp), %eax
        jle     .L6
        movzbl  -21(%rbp), %edx
        movsbw  %dl, %cx
        movl    %ecx, %eax
        sall    $7, %eax
        subl    %ecx, %eax
        shrw    $8, %ax
        movl    %eax, %ecx
        sarb    $5, %cl
        movl    %edx, %eax
        sarb    $7, %al
        subl    %eax, %ecx
        movl    %ecx, %eax
        sall    $6, %eax
        addl    %ecx, %eax
        movl    %edx, %ecx
        subl    %eax, %ecx
        movl    %ecx, %eax
        addl    $66, %eax
        movb    %al, -21(%rbp)
.L4:
        movl    -28(%rbp), %eax
        cmpl    -52(%rbp), %eax
        jl      .L7
        movq    -80(%rbp), %rax
        movl    -52(%rbp), %edx
        movl    %edx, (%rax)
        movq    -64(%rbp), %rax
        addq    $72, %rsp
        popq    %rbx
        popq    %rbp
        ret
