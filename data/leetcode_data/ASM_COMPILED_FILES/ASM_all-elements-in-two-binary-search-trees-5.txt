func_1:
        pushq   %rbp
        movq    %rsp, %rbp
        subq    $32, %rsp
        movq    %rdi, -8(%rbp)
        movq    %rsi, -16(%rbp)
        movq    %rdx, -24(%rbp)
        movq    -8(%rbp), %rax
        movq    8(%rax), %rax
        testq   %rax, %rax
        je      .L2
        movq    -8(%rbp), %rax
        movq    8(%rax), %rax
        movq    -24(%rbp), %rdx
        movq    -16(%rbp), %rcx
        movq    %rcx, %rsi
        movq    %rax, %rdi
        call    func_1
.L2:
        movq    -24(%rbp), %rax
        movl    (%rax), %eax
        leal    1(%rax), %edx
        movq    -24(%rbp), %rax
        movl    %edx, (%rax)
        movq    -24(%rbp), %rax
        movl    (%rax), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -16(%rbp), %rax
        movq    (%rax), %rax
        movq    %rdx, %rsi
        movq    %rax, %rdi
        call    realloc
        movq    %rax, %rdx
        movq    -16(%rbp), %rax
        movq    %rdx, (%rax)
        movq    -16(%rbp), %rax
        movq    (%rax), %rdx
        movq    -24(%rbp), %rax
        movl    (%rax), %eax
        cltq
        salq    $2, %rax
        subq    $4, %rax
        addq    %rax, %rdx
        movq    -8(%rbp), %rax
        movl    (%rax), %eax
        movl    %eax, (%rdx)
        movq    -8(%rbp), %rax
        movq    16(%rax), %rax
        testq   %rax, %rax
        je      .L4
        movq    -8(%rbp), %rax
        movq    16(%rax), %rax
        movq    -24(%rbp), %rdx
        movq    -16(%rbp), %rcx
        movq    %rcx, %rsi
        movq    %rax, %rdi
        call    func_1
.L4:
        nop
        leave
        ret
func_2:
        pushq   %rbp
        movq    %rsp, %rbp
        movq    %rdi, -24(%rbp)
        movl    %esi, -28(%rbp)
        movq    %rdx, -40(%rbp)
        movl    %ecx, -32(%rbp)
        movq    %r8, -48(%rbp)
        movl    $0, -4(%rbp)
        movl    $0, -8(%rbp)
        movl    $0, -12(%rbp)
        jmp     .L6
.L10:
        movl    -4(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %edx
        movl    -8(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rcx
        movq    -40(%rbp), %rax
        addq    %rcx, %rax
        movl    (%rax), %eax
        cmpl    %eax, %edx
        jge     .L7
        movl    -4(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -4(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        addq    %rdx, %rax
        movl    -12(%rbp), %edx
        movslq  %edx, %rdx
        leaq    0(,%rdx,4), %rcx
        movq    -48(%rbp), %rdx
        addq    %rcx, %rdx
        movl    (%rax), %eax
        movl    %eax, (%rdx)
        jmp     .L8
.L7:
        movl    -8(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -8(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -40(%rbp), %rax
        addq    %rdx, %rax
        movl    -12(%rbp), %edx
        movslq  %edx, %rdx
        leaq    0(,%rdx,4), %rcx
        movq    -48(%rbp), %rdx
        addq    %rcx, %rdx
        movl    (%rax), %eax
        movl    %eax, (%rdx)
.L8:
        addl    $1, -12(%rbp)
.L6:
        movl    -4(%rbp), %eax
        cmpl    -28(%rbp), %eax
        jge     .L11
        movl    -8(%rbp), %eax
        cmpl    -32(%rbp), %eax
        jl      .L10
        jmp     .L11
.L12:
        movl    -4(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -4(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        leaq    (%rdx,%rax), %rcx
        movl    -12(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -12(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -48(%rbp), %rax
        addq    %rax, %rdx
        movl    (%rcx), %eax
        movl    %eax, (%rdx)
.L11:
        movl    -4(%rbp), %eax
        cmpl    -28(%rbp), %eax
        jl      .L12
        jmp     .L13
.L14:
        movl    -8(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -8(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -40(%rbp), %rax
        leaq    (%rdx,%rax), %rcx
        movl    -12(%rbp), %eax
        leal    1(%rax), %edx
        movl    %edx, -12(%rbp)
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -48(%rbp), %rax
        addq    %rax, %rdx
        movl    (%rcx), %eax
        movl    %eax, (%rdx)
.L13:
        movl    -8(%rbp), %eax
        cmpl    -32(%rbp), %eax
        jl      .L14
        nop
        popq    %rbp
        ret
func_3:
        pushq   %rbp
        movq    %rsp, %rbp
        subq    $64, %rsp
        movq    %rdi, -40(%rbp)
        movq    %rsi, -48(%rbp)
        movq    %rdx, -56(%rbp)
        movq    $0, -8(%rbp)
        movq    $0, -16(%rbp)
        movq    $0, -24(%rbp)
        movl    $0, -28(%rbp)
        movl    $0, -32(%rbp)
        cmpq    $0, -40(%rbp)
        je      .L16
        leaq    -28(%rbp), %rdx
        leaq    -16(%rbp), %rcx
        movq    -40(%rbp), %rax
        movq    %rcx, %rsi
        movq    %rax, %rdi
        call    func_1
.L16:
        cmpq    $0, -48(%rbp)
        je      .L17
        leaq    -32(%rbp), %rdx
        leaq    -24(%rbp), %rcx
        movq    -48(%rbp), %rax
        movq    %rcx, %rsi
        movq    %rax, %rdi
        call    func_1
.L17:
        movl    -28(%rbp), %edx
        movl    -32(%rbp), %eax
        addl    %eax, %edx
        movq    -56(%rbp), %rax
        movl    %edx, (%rax)
        movq    -56(%rbp), %rax
        movl    (%rax), %eax
        cltq
        salq    $2, %rax
        movq    %rax, %rdi
        call    malloc
        movq    %rax, -8(%rbp)
        movl    -32(%rbp), %ecx
        movq    -24(%rbp), %rdx
        movl    -28(%rbp), %esi
        movq    -16(%rbp), %rax
        movq    -8(%rbp), %rdi
        movq    %rdi, %r8
        movq    %rax, %rdi
        call    func_2
        movq    -16(%rbp), %rax
        movq    %rax, %rdi
        call    free
        movq    -24(%rbp), %rax
        movq    %rax, %rdi
        call    free
        movq    -8(%rbp), %rax
        leave
        ret
