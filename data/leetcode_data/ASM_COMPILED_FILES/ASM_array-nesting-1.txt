func_1:
        pushq   %rbp
        movq    %rsp, %rbp
        subq    $48, %rsp
        movq    %rdi, -40(%rbp)
        movl    %esi, -44(%rbp)
        movl    -44(%rbp), %eax
        cltq
        salq    $2, %rax
        movq    %rax, %rdi
        call    malloc
        movq    %rax, -24(%rbp)
        movl    -44(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        movl    $-1, %esi
        movq    %rax, %rdi
        call    memset
        movl    $0, -4(%rbp)
        movl    $0, -8(%rbp)
        jmp     .L2
.L5:
        movl    -8(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -40(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %eax
        movl    %eax, -12(%rbp)
        movl    $1, -16(%rbp)
        movl    -8(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %edx
        addl    $1, %edx
        movl    %edx, (%rax)
        jmp     .L3
.L4:
        movl    -12(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %edx
        addl    $1, %edx
        movl    %edx, (%rax)
        addl    $1, -16(%rbp)
        movl    -12(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -40(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %eax
        movl    %eax, -12(%rbp)
.L3:
        movl    -12(%rbp), %eax
        cltq
        leaq    0(,%rax,4), %rdx
        movq    -24(%rbp), %rax
        addq    %rdx, %rax
        movl    (%rax), %eax
        cmpl    $-1, %eax
        je      .L4
        movl    -16(%rbp), %eax
        cmpl    %eax, -4(%rbp)
        cmovge  -4(%rbp), %eax
        movl    %eax, -4(%rbp)
        addl    $1, -8(%rbp)
.L2:
        movl    -8(%rbp), %eax
        cmpl    -44(%rbp), %eax
        jl      .L5
        movq    -24(%rbp), %rax
        movq    %rax, %rdi
        call    free
        movl    -4(%rbp), %eax
        leave
        ret
